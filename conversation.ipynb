{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to Add Conversation Memory to Your Chatbot\n",
    "==============================================\n",
    "\n",
    "A tutorial for adding conversation memory to a chatbot.\n",
    "\n",
    "## What is it?\n",
    "\n",
    "This article demonstrates how to take an existing chatbot that can answer questions from a document of private knowledge and enhance it to allow for full conversation, back and forth, between the chatbot and user. The chatbot will maintain context of the conversation, remembering what the user has previously asked, and what responses the chatbot has previously replied with.\n",
    "\n",
    "The chatbot uses an AI large language model (LLM) with retrieval augmented generation (RAG) to answer questions from a PDF or HTML file. By enhancing it with a history of the conversation, the user can ask continuous questions without having to repeat details of the subject."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What can it do?\n",
    "\n",
    "The chatbot capable of remembering the history of a conversation is built upon the existing [project](https://github.com/primaryobjects/chatbot/blob/main/chatbot.ipynb) that creates an LLM-based chatbot from a private knowledge-base.\n",
    "\n",
    "The origial [chatbot](https://github.com/primaryobjects/chatbot/blob/main/chatbot.ipynb) could answer one-off questions from content within the ingested document. However, if the user were to ask follow-up questions, the chatbot would not maintain a history and would perform an entirely new search in the document to try and answer the question. If specific keywords were missing from the query, the chatbot would be unlikely to give a suitable response.\n",
    "\n",
    "This enhanced version of the chatbot works by maintaining a conversation history using an in-memory list. The memory list could also be adapted to use a distributed memory, such as [Redis](https://pypi.org/project/redis/). At each round of interaction with the chatbot, we store the user's query, the context knowledge found within the document, and the chatbot's response. This list of converation items is provided in the prompt to the chatbot as additional context during each subsequent query in the conversation.\n",
    "\n",
    "## How does it work?\n",
    "\n",
    "The chatbot enhanced with conversation memory works using the following steps.\n",
    "\n",
    "1. Use the original chatbot to load a PDF or HTML file of private knowledge.\n",
    "2. The user asks a query.\n",
    "3. Retrieve a search result from the document that best matches the user's query to use as context.\n",
    "4. The LLM uses the context to answer the question and respond to the user.\n",
    "5. Store the user's query, context, and LLM response in a list.\n",
    "6. Upon subsequent queries, include the entire conversation list as additional context.\n",
    "\n",
    "Note, the prompt will grow larger at each round of the conversation, due to the expanding length of the conversation history. It's important to keep note of the length of the conversation and the maximum token size that the LLM model can maintain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the chatbot\n",
    "\n",
    "We begin by defining the methods for the original LLM chatbot. The methods include processing a PDF or HTML file and loading into a knowledge-base, searching the document for a best matching result to the user's query, and calling the LLM for a response while using the search result as context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install numpy pandas scikit-learn nltk PyPDF2 Cohere\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)\n",
    "\n",
    "NUMBER_OF_MATCHES = 3 # Number of matching items to provide as context to the AI\n",
    "CHUNK_SIZE = 99999 # Size of each matching item (99999 = entire document as context)\n",
    "\n",
    "import os\n",
    "import nltk\n",
    "import cohere\n",
    "import requests\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from PyPDF2 import PdfReader\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "nltk.download('punkt')\n",
    "ps = PorterStemmer()\n",
    "\n",
    "def process_text(text, chunk_size=CHUNK_SIZE):\n",
    "    sentences = sent_tokenize(text)\n",
    "    original_chunks = []\n",
    "    processed_chunks = []\n",
    "    chunk = \"\"\n",
    "    for sentence in sentences:\n",
    "        if len(chunk) + len(sentence) > chunk_size:\n",
    "            original_chunks.append(chunk)\n",
    "            processed_chunks.append(' '.join([ps.stem(word) for word in chunk.split()]))\n",
    "            chunk = sentence\n",
    "        else:\n",
    "            chunk += \" \" + sentence\n",
    "    if chunk:\n",
    "        original_chunks.append(chunk)\n",
    "        processed_chunks.append(' '.join([ps.stem(word) for word in chunk.split()]))\n",
    "    return original_chunks, processed_chunks\n",
    "\n",
    "def read_pdf(file_path):\n",
    "    with open(file_path, 'rb') as file:\n",
    "        reader = PdfReader(file)\n",
    "        text = ''\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text()\n",
    "    return process_text(text)\n",
    "\n",
    "def read_html(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        soup = BeautifulSoup(file, 'html.parser')\n",
    "        text = soup.get_text()\n",
    "        return process_text(text)\n",
    "\n",
    "def read_txt(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        text = file.read()\n",
    "        return process_text(text)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "documents = []  # This will hold all processed documents\n",
    "original_documents = []  # This will hold all original documents\n",
    "vectors = None\n",
    "\n",
    "def process_and_add_document(file_path, file_type):\n",
    "    if file_type == 'pdf':\n",
    "        original_chunks, processed_chunks = read_pdf(file_path)\n",
    "    elif file_type == 'html':\n",
    "        original_chunks, processed_chunks = read_html(file_path)\n",
    "    elif file_type == 'txt':\n",
    "        original_chunks, processed_chunks = read_txt(file_path)\n",
    "    else:\n",
    "        raise ValueError('Unsupported file type')\n",
    "    \n",
    "    original_documents.extend(original_chunks)  # Store the original text chunks\n",
    "    vectors = add_document(processed_chunks)\n",
    "    return vectors\n",
    "\n",
    "def add_document(text):\n",
    "    documents.extend(text)\n",
    "    vectors = vectorizer.fit_transform(documents)\n",
    "    return vectors\n",
    "\n",
    "def find_best_matches(query, top_n=NUMBER_OF_MATCHES):\n",
    "    query_processed = process_text(query)[1]  # Get the processed version of the query\n",
    "    query_vector = vectorizer.transform(query_processed)\n",
    "    similarities = (query_vector * vectors.T).toarray()\n",
    "    best_match_indices = similarities.argsort()[0][-top_n:][::-1]  # Get the indices of the top N matches\n",
    "    return [original_documents[i] for i in best_match_indices], [documents[i] for i in best_match_indices]\n",
    "\n",
    "co = cohere.ClientV2(os.getenv('COHERE_API_KEY'))\n",
    "\n",
    "def get_cohere_response(query, context):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are an AI assistant. Use the provided context to answer the user's query accurately in a short and concise response. Do not generate information that is not present in the context. If the context does not contain the answer, inform the user that the information is not available.\"},\n",
    "        {\"role\": \"system\", \"content\": context},\n",
    "        {\"role\": \"user\", \"content\": query}\n",
    "    ]\n",
    "\n",
    "    response = co.chat(\n",
    "        model='command-r-plus-08-2024',\n",
    "        messages=messages\n",
    "    )\n",
    "    return response.message.content[0].text.strip()\n",
    "\n",
    "def reset_database():\n",
    "    global documents, original_documents, vectors\n",
    "    documents = []\n",
    "    original_documents = []\n",
    "    vectors = None\n",
    "\n",
    "def initialize(file_name):\n",
    "    file_type = file_name.split('.')[-1]\n",
    "    return process_and_add_document(file_name, file_type)\n",
    "\n",
    "def process_chat(user_query, is_debug = False):\n",
    "    original_best_matches, processed_best_matches = find_best_matches(user_query)\n",
    "    context = \"\\n\\n\".join(original_best_matches)  # Concatenate the top 3 best matches as context\n",
    "    if is_debug:\n",
    "        print(f\"Context: {context}\")\n",
    "    response = get_cohere_response(user_query, context)\n",
    "    return response, context\n",
    "\n",
    "def chat(user_query, is_debug = False):\n",
    "    return process_chat(user_query, is_debug)[0]\n",
    "\n",
    "# Download the sample files from the provided URLs.\n",
    "def download_sample_files():\n",
    "    sample_files = [\n",
    "        {\n",
    "            \"url\": \"https://www.ipcc.ch/report/ar6/wg1/downloads/outreach/IPCC_AR6_WGI_SummaryForAll.pdf\",\n",
    "            \"file_name\": \"climatechange.pdf\"\n",
    "        },\n",
    "        {\n",
    "            \"url\": \"https://medium.com/illumination/i-tried-10-decaf-coffees-as-a-first-time-coffee-drinker-heres-what-i-found-a8c5fb93a40e\",\n",
    "            \"file_name\": \"coffee.html\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    for file in sample_files:\n",
    "        response = requests.get(file[\"url\"])\n",
    "        with open(file[\"file_name\"], 'wb') as f:\n",
    "            # Save the file to the same directory as the executing script.\n",
    "\n",
    "            f.write(response.content)\n",
    "\n",
    "    # Return a list of file names.\n",
    "    return [file[\"file_name\"] for file in sample_files]\n",
    "\n",
    "# Initialize the chatbot.\n",
    "file_names = download_sample_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the original chatbot\n",
    "\n",
    "Let's see an example of the original chatbot answering questions in a conversation. Notice how when the subject matter is included in the query, a suitable response is returned from the LLM. However, when subsequent questions are asked about the original topic, the LLM has no prior knowledge and is unable to answer the questions correctly.\n",
    "\n",
    "The original chatbot, quite simply, forgets the conversation topic!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first five authors of the report are:\n",
      "1. Govindasamy Bala\n",
      "2. Deliang Chen\n",
      "3. Tamsin Edwards\n",
      "4. Sandro Fuzzi\n",
      "5. Thian Yew Gan\n"
     ]
    }
   ],
   "source": [
    "# Setup chatbot.\n",
    "reset_database()\n",
    "vectors = initialize('climatechange.pdf')\n",
    "\n",
    "# Ask a conversation of questions on the topic of authors.\n",
    "print(chat('Who are the first 5 authors from the report?'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Asking follow-up questions\n",
    "\n",
    "Next, let's ask follow-up questions on the same topic as you would in a natural conversation. We won't mention the topic \"authors\" again as we want the chatbot to remember the history of the conversation. Of course, since we have not added memory yet, it won't remember!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first item in the list is the Intergovernmental Panel on Climate Change (IPCC).\n",
      "The last item in the list is \"Thank you to everyone who contributed to this summary.\"\n"
     ]
    }
   ],
   "source": [
    "print(chat('Which one is the first?'))\n",
    "print(chat('Which one is last?'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What went wrong?\n",
    "\n",
    "In the above example, you can see how we asked three continuous questions about the authors of the document.\n",
    "\n",
    "The first question, explicitly mentioned \"authors\" as the subject of the query. A suitable response was provided that listed all of the authors found in the document. However, the second and third questions continued upon the same topic. The user asked which was the first, followed by how many were there in total - without ever mentioning the topic \"author\". Since the original "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enhancing the chatbot with memory\n",
    "\n",
    "We can update the chatbot to include a memory of the conversation by saving the user's query, context, and LLM response at each round. We can later load the conversation and include it as additional context in the prompt to the LLM.\n",
    "\n",
    "Including within the prompt the prior conversation effectively gives the LLM a memory!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding to the conversation\n",
    "\n",
    "We can define a new method for saving the current query from the user as an item in the conversation. Each saved item will include the query, context, and LLM response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to store the conversation history\n",
    "conversation_history = []\n",
    "\n",
    "def add_to_conversation(user_input, search_result_context, llm_response):\n",
    "    \"\"\"\n",
    "    Adds a conversation entry to the conversation history.\n",
    "\n",
    "    Parameters:\n",
    "    user_input (str): The user's input.\n",
    "    search_result_context (str): The context found from the search results.\n",
    "    llm_response (str): The response from the LLM.\n",
    "    \"\"\"\n",
    "    conversation_entry = {\n",
    "        \"user_input\": user_input,\n",
    "        \"search_result_context\": search_result_context,\n",
    "        \"llm_response\": llm_response\n",
    "    }\n",
    "    conversation_history.append(conversation_entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the conversation\n",
    "\n",
    "Next, we can define new enhanced chat method that saves the conversation using the method that we've just defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conversation(user_query, is_debug = False):\n",
    "    # Append the conversation history to a single string to be used in the LLM prompt context.\n",
    "    history = \"\\n\\n=========================\\n\\n\".join([f\"User: {entry['user_input']}\\nSearch Result Context: {entry['search_result_context']}\\nLLM Response: {entry['llm_response']}\" for entry in conversation_history])\n",
    "\n",
    "    # Format the query to include the entire conversation history.\n",
    "    query = f\"This is the entire conversation up to this point. Use this as additional context when answering the question from the user.\\n CONTEXT: {history}\\n\\n\"\n",
    "    query += f\"\\n\\n=========================\\n\\n USER QUERY: {user_query}\"\n",
    "\n",
    "    # Get the response from the LLM.\n",
    "    response, context = process_chat(query, is_debug)\n",
    "\n",
    "    # Add the conversation entry to the history.\n",
    "    add_to_conversation(user_query, context, response)\n",
    "\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's try that again\n",
    "\n",
    "We can now run the chatbot enhanced with a conversation memory, and see how it can answer follow-up questions in the conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first five authors of the report are:\n",
      "1. Govindasamy Bala\n",
      "2. Deliang Chen\n",
      "3. Tamsin Edwards\n",
      "4. Sandro Fuzzi\n",
      "5. Thian Yew Gan\n",
      "Govindasamy Bala is the first author of the report.\n",
      "The fifth author is Thian Yew Gan.\n"
     ]
    }
   ],
   "source": [
    "conversation_history = []\n",
    "\n",
    "# Ask a conversation of questions on the topic of authors.\n",
    "print(conversation('Who are the first 5 authors from the report?'))\n",
    "print(conversation('Which one is the first?'))\n",
    "print(conversation('Which one is fifth?'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verifying the conversation history\n",
    "\n",
    "To verify the chatbot is actually keep track of the conversation, let's ask it what the first question was. The chatbot must use the conversation memory as context in order to answer this question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first question you asked was, \"Who are the first 5 authors from the report?\"\n"
     ]
    }
   ],
   "source": [
    "print(conversation('What was the first question that I asked?'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remembering the topic\n",
    "\n",
    "Does the chatbot remember the topic of the conversation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The topic of this conversation is the first five authors of the report.\n"
     ]
    }
   ],
   "source": [
    "print(conversation('What is the topic of this conversation?'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
