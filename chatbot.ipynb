{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to Create a Chatbot with a Private Knowledge-Base Using RAG\n",
    "==============\n",
    "\n",
    "A tutorial for creating a chatbot that uses a private knowledge-base with retrieval augmented generation.\n",
    "\n",
    "## What is it?\n",
    "\n",
    "This is an easy tutorial for creating a basic chatbot with a private knowledge-base. The chatbot can answer questions related to a specific business, product, or domain. Unlike general chatbots (ChatGPT, etc.), a personal chatbot trained using retrieval augmented generation (RAG) can answer questions that are specific to a domain. For example, the chatbot could answer questions from your company's support technical support; or it could have specific knowledge about your business brochure, or perhaps about a specific person (such as yourself), or even about a personal hobby.\n",
    "\n",
    "## How does it work?\n",
    "\n",
    "The chatbot works by allowing users to upload documents (text files, PDF documents, HTML pages) from which the chatbot will utilize the content for constructing its responses.\n",
    "\n",
    "1. Each document is [stemmed](https://www.ibm.com/think/topics/stemming) and split into chunks (i.e., sentences and paragraphs).\n",
    "2. The chunks are converted into a numeric vector.\n",
    "3. The user enters a query for the chatbot.\n",
    "4. The user's query is stemmed and converted into a numeric vector.\n",
    "5. The user's query is matched agaist the database of content using a text similarity algorithm.\n",
    "6. The top-N matches are included in the prompt as context, along with the user's query, and sent to an LLM.\n",
    "7. The LLM uses the context to answer the question and respond to the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install numpy pandas scikit-learn nltk PyPDF2 Cohere\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)\n",
    "\n",
    "NUMBER_OF_MATCHES = 3 # Number of matching items to provide as context to the AI\n",
    "CHUNK_SIZE = 99999 # Size of each matching item (99999 = entire document as context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding documents to the knowledge-base\n",
    "\n",
    "Content can be added into the knowledge-base by providing text, PDF, or HTML files. The following methods read the associated files, extract the text, and return the stemmed and processed chunk data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from PyPDF2 import PdfReader\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "nltk.download('punkt')\n",
    "ps = PorterStemmer()\n",
    "\n",
    "def process_text(text, chunk_size=CHUNK_SIZE):\n",
    "    sentences = sent_tokenize(text)\n",
    "    original_chunks = []\n",
    "    processed_chunks = []\n",
    "    chunk = \"\"\n",
    "    for sentence in sentences:\n",
    "        if len(chunk) + len(sentence) > chunk_size:\n",
    "            original_chunks.append(chunk)\n",
    "            processed_chunks.append(' '.join([ps.stem(word) for word in chunk.split()]))\n",
    "            chunk = sentence\n",
    "        else:\n",
    "            chunk += \" \" + sentence\n",
    "    if chunk:\n",
    "        original_chunks.append(chunk)\n",
    "        processed_chunks.append(' '.join([ps.stem(word) for word in chunk.split()]))\n",
    "    return original_chunks, processed_chunks\n",
    "\n",
    "def read_pdf(file_path):\n",
    "    with open(file_path, 'rb') as file:\n",
    "        reader = PdfReader(file)\n",
    "        text = ''\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text()\n",
    "    return process_text(text)\n",
    "\n",
    "def read_html(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        soup = BeautifulSoup(file, 'html.parser')\n",
    "        text = soup.get_text()\n",
    "        return process_text(text)\n",
    "\n",
    "def read_txt(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        text = file.read()\n",
    "        return process_text(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing content\n",
    "\n",
    "After the documents have been processed with stemming and divided into chunks, they are converted to numeric vectors and added into a database in memory.\n",
    "\n",
    "## Finding the best matches\n",
    "\n",
    "The LLM requires the most relevant context in order to provide an accurate response. To identify the best matching knowledge from the processed content, a similarity algorithm is executed against the user query and the document database. The top-N best matches are returned, along with their original text, for use within the prompt that will be sent to the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "documents = []  # This will hold all processed documents\n",
    "original_documents = []  # This will hold all original documents\n",
    "vectors = None\n",
    "\n",
    "def process_and_add_document(file_path, file_type):\n",
    "    if file_type == 'pdf':\n",
    "        original_chunks, processed_chunks = read_pdf(file_path)\n",
    "    elif file_type == 'html':\n",
    "        original_chunks, processed_chunks = read_html(file_path)\n",
    "    elif file_type == 'txt':\n",
    "        original_chunks, processed_chunks = read_txt(file_path)\n",
    "    else:\n",
    "        raise ValueError('Unsupported file type')\n",
    "    \n",
    "    original_documents.extend(original_chunks)  # Store the original text chunks\n",
    "    vectors = add_document(processed_chunks)\n",
    "    return vectors\n",
    "\n",
    "def add_document(text):\n",
    "    documents.extend(text)\n",
    "    vectors = vectorizer.fit_transform(documents)\n",
    "    return vectors\n",
    "\n",
    "def find_best_matches(query, top_n=NUMBER_OF_MATCHES):\n",
    "    query_processed = process_text(query)[1]  # Get the processed version of the query\n",
    "    query_vector = vectorizer.transform(query_processed)\n",
    "    similarities = (query_vector * vectors.T).toarray()\n",
    "    best_match_indices = similarities.argsort()[0][-top_n:][::-1]  # Get the indices of the top N matches\n",
    "    return [original_documents[i] for i in best_match_indices], [documents[i] for i in best_match_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing the prompt for the LLM\n",
    "\n",
    "The next step is to construct a prompt for the LLM to respond to the user. This is the core behind the chatbot. To do this, we first provide a system prompt for the LLM that explains they are an assistant and will use the context provided to formulate a response. They should not use general knowledge from outside the scope of the context.\n",
    "\n",
    "The complete prompt sent to the LLM includes the instructional system prompt, the best matches from the document database as context, and the user's query.\n",
    "\n",
    "## Calling the LLM\n",
    "\n",
    "Once the prompt is constructed, the Cohere LLM is called via its API endpoint and a response is returned to the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cohere\n",
    "\n",
    "co = cohere.ClientV2(os.getenv('COHERE_API_KEY'))\n",
    "\n",
    "def get_cohere_response(query, context):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are an AI assistant. Use the provided context to answer the user's query accurately in a short and concise response. Do not generate information that is not present in the context. If the context does not contain the answer, inform the user that the information is not available.\"},\n",
    "        {\"role\": \"system\", \"content\": context},\n",
    "        {\"role\": \"user\", \"content\": query}\n",
    "    ]\n",
    "\n",
    "    response = co.chat(\n",
    "        model='command-r-plus-08-2024',\n",
    "        messages=messages\n",
    "    )\n",
    "    return response.message.content[0].text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all together\n",
    "\n",
    "The method `chat()` shown below provides an example of putting all the pieces together. A call is made to `find_best_matches()` to locate the best context to use with the user's query before sending to the LLM within the prompt. The Cohere LLM is called and the response is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_database():\n",
    "    global documents, original_documents, vectors\n",
    "    documents = []\n",
    "    original_documents = []\n",
    "    vectors = None\n",
    "\n",
    "def initialize(file_name):\n",
    "    file_type = file_name.split('.')[-1]\n",
    "    return process_and_add_document(file_name, file_type)\n",
    "\n",
    "def chat(user_query, is_debug = False):\n",
    "    original_best_matches, processed_best_matches = find_best_matches(user_query)\n",
    "    context = \"\\n\\n\".join(original_best_matches)  # Concatenate the top 3 best matches as context\n",
    "    if is_debug:\n",
    "        print(f\"Context: {context}\")\n",
    "    response = get_cohere_response(user_query, context)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "The example below initializes the knowledge-base using a PDF document `climatechange.pdf`. The document will be stemmed, chunked, vectorized, and added into the memory database. The user then provides a query for the chatbot from which we locate the best matches from the document database and return the matches as context for the user's query in the prompt to the LLM. Fially, the response is printed to the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The report was written by members of the Working Group I Technical Support Unit (WGI TSU) and several authors of the report. The authors are:\n",
      "\n",
      "- Sarah Connors (WGI TSU)\n",
      "- Sophie Berger (WGI TSU)\n",
      "- Clotilde Péan (WGI TSU)\n",
      "- Govindasamy Bala (Chapter 4 author)\n",
      "- Nada Caud (WGI TSU)\n",
      "- Deliang Chen (Chapter 1 author)\n",
      "- Tamsin Edwards (Chapter 9 author)\n",
      "- Sandro Fuzzi (Chapter 6 author)\n",
      "- Thian Yew Gan (Chapter 8 author)\n",
      "- Melissa Gomis (WGI TSU)\n",
      "- Ed Hawkins (Chapter 1 author)\n",
      "- Richard Jones (Atlas Chapter author)\n",
      "- Robert Kopp (Chapter 9 author)\n",
      "- Katherine Leitzell (WGI TSU)\n",
      "- Elisabeth Lonnoy (WGI TSU)\n",
      "- Douglas Maraun (Chapter 10 author)\n",
      "- Valérie Masson-Delmotte (WGI Co-Chair)\n",
      "- Tom Maycock (WGI TSU)\n",
      "- Anna Pirani (WGI TSU)\n",
      "- Roshanka Ranasinghe (Chapter 12 author)\n",
      "- Joeri Rogelj (Chapter 5 author)\n",
      "- Alex C. Ruane (Chapter 12 author)\n",
      "- Sophie Szopa (Chapter 6 author)\n",
      "- Panmao Zhai (WGI Co-Chair)\n"
     ]
    }
   ],
   "source": [
    "reset_database()\n",
    "vectors = initialize('climatechange.pdf')\n",
    "response = chat('Who are the authors of the report?')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Completed Chatbot\n",
    "\n",
    "By providing a continuous loop to chat with the chatbot, the user can repeatedly enter queries for the chatbot to respond to. Each query performs the same process of locating the best matching context and calling the LLM with the constructed prompt.\n",
    "\n",
    "In this scenario, an article about reviews of [coffee](https://medium.com/illumination/i-tried-10-decaf-coffees-as-a-first-time-coffee-drinker-heres-what-i-found-a8c5fb93a40e?sk=03a1bb8109f779521d9ffec8f5f275ae) are provided as knowledge to the LLM. This allows it to answer questions related specifically to an article written by the author."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "User: \"Which coffee has the highest rating?\"\n",
      "AI: The highest-rated coffee in the review is Merit Coffee Espresso Decaf.\n",
      "--------------------\n",
      "User: \"What was the author afraid that coffee might do to them?\"\n",
      "AI: The author was afraid that coffee might stain their teeth, upset their stomach, or make them jittery.\n",
      "--------------------\n",
      "User: \"Who is the author?\"\n",
      "AI: The author of the article is Kory Becker.\n",
      "--------------------\n",
      "User: \"How was the review for Folgers decaf?\"\n",
      "AI: The Folgers Instant Decaf coffee was ranked 8th in the list. It was described as having a bitter and dark flavor, with no impact on the stomach or caffeine effect. The coffee flakes dissolved quickly in water, and the price was noted as being a bit more expensive compared to some other options.\n",
      "Exiting the chat. Goodbye!\n"
     ]
    }
   ],
   "source": [
    "reset_database()\n",
    "vectors = initialize('coffee.html')\n",
    "while True:\n",
    "    user_query = input(\"Enter your query (type 'quit' or 'exit' to stop): \")\n",
    "    if user_query.lower() in ['quit', 'exit']:\n",
    "        print(\"Exiting the chat. Goodbye!\")\n",
    "        break\n",
    "    print('--------------------')\n",
    "    print(f\"User: \\\"{user_query}\\\"\")\n",
    "    response = chat(user_query)\n",
    "    print(\"AI:\", response, flush=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
